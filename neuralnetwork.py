# -*- coding: utf-8 -*-
"""NeuralNetwork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19nVdiHYvRwNAasVyIBgevg6hs0YKL_L7
"""

import numpy as np


def sigmoid(x):
  return 1/(1+np.exp(-x))


class NeuralNetwork:
  def __init__(self,input_dim=2,hidden_dim=2,output_dim=2):
    self.W = 0.1 * np.random.rand(input_dim,output_dim)
    self.V = 0.1 * np.random.rand(hidden_dim,output_dim)


  #expects X to be ( n * input_dim ) matrix
  def forward(self,X):
    self.X = X

    self.H_in = np.dot(X,self.W)
    self.H = sigmoid(self.H_in)

    self.Y_in = np.dot(self.H,self.V)
    self.Y = sigmoid(self.Y_in)

    return self.Y


  #expects T to be a ( n * output_dim ) matrix
  def backward(self,T):
    E = self.Y - T
    E_sq = E**2
    self.L = np.sum(E_sq,axis=1,keepdims=True)
    grad_Y = 2*E

    grad_Y_in = (self.Y)*(1-self.Y)*grad_Y
    grad_V = np.dot((self.H).T,grad_Y_in)
    grad_H = np.dot(grad_Y_in,(self.V).T)

    grad_H_in = (self.H)*(1-self.H)*grad_H
    grad_W = np.dot((self.X).T,grad_H_in)

    return grad_W,grad_V

X = np.array([[3.,1.]])
T = np.array([[1.,0.]])

W = np.array([[6.,-3.],[-2.,5.]])
V = np.array([[1.,-2.],[0.25,2.]])

net = NeuralNetwork()

net.W = W.copy()
net.V = V.copy()

# first training instance
X = np.array( [[3., 1. ]])
T = np.array( [[1., 0. ]])
net.forward(X)
g_W_1, g_V_1 = net.backward(T)
# initial loss
init_loss_1 = net.L

# second training instance
X = np.array( [[-1., 4. ]])
T = np.array( [[0., 1. ]])
net.forward(X)
g_W_2, g_V_2 = net.backward(T)
# initial loss
init_loss_2 = net.L

g_W, g_V = g_W_1 + g_W_2, g_V_1 + g_V_2

alpha = 0.5 # very large for demonstration

net.W -= alpha * g_W
net.V -= alpha * g_V

print(net.V)

